# Prune Before Converged

In this work, we we propose a structured pruning method to efficiently train Transformer models for neural machine translation. We construct experiments on two widely-used machine translation datasets, and results show that pruning early before convergence significantly saves total training time while keeps comparable performance.